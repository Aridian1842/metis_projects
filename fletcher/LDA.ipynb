{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with gensim\n",
    "We'll try out [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) in [gensim](http://radimrehurek.com/gensim/index.html) on the [20 Newsgroups dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html) with some simple preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-0.12.4-cp27-none-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (2.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.5MB 179kB/s \n",
      "\u001b[?25hCollecting scipy>=0.7.0 (from gensim)\n",
      "  Downloading scipy-0.17.1-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (21.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 21.1MB 24kB/s \n",
      "\u001b[?25hRequirement already up-to-date: six>=1.5.0 in /Users/peter/anaconda/lib/python2.7/site-packages (from gensim)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.3.3.tar.gz\n",
      "Requirement already up-to-date: numpy>=1.3 in /Users/peter/anaconda/lib/python2.7/site-packages (from gensim)\n",
      "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading boto-2.40.0-py2.py3-none-any.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 335kB/s \n",
      "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "Collecting requests (from smart-open>=1.2.1->gensim)\n",
      "  Downloading requests-2.10.0-py2.py3-none-any.whl (506kB)\n",
      "\u001b[K    100% |████████████████████████████████| 512kB 720kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/peter/Library/Caches/pip/wheels/e2/05/25/61b5f9984feaaebaa60cf7800691a21a03db6701689bcc9075\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/peter/Library/Caches/pip/wheels/31/9c/20/996d65ca104cbca940b1b053299b68459391c01c774d073126\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: scipy, boto, bz2file, requests, smart-open, gensim\n",
      "  Found existing installation: scipy 0.17.0\n",
      "\u001b[33m    DEPRECATION: Uninstalling a distutils installed project (scipy) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling scipy-0.17.0:\n",
      "      Successfully uninstalled scipy-0.17.0\n",
      "  Found existing installation: boto 2.38.0\n",
      "\u001b[33m    DEPRECATION: Uninstalling a distutils installed project (boto) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling boto-2.38.0:\n",
      "      Successfully uninstalled boto-2.38.0\n",
      "  Found existing installation: requests 2.9.1\n",
      "\u001b[33m    DEPRECATION: Uninstalling a distutils installed project (requests) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling requests-2.9.1:\n",
      "      Successfully uninstalled requests-2.9.1\n",
      "Successfully installed boto-2.40.0 bz2file-0.98 gensim-0.12.4 requests-2.10.0 scipy-0.17.1 smart-open-1.3.3\n",
      "\u001b[33mYou are using pip version 8.1.1, however version 8.1.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retain only a subset of the 20 categories in the original 20 Newsgroups Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sklearn.datasets.twenty_newsgroups:Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# Set categories\n",
    "categories = ['alt.atheism', 'comp.graphics', 'rec.sport.baseball']#, \n",
    "              #'rec.motorcycles', 'sci.space', 'talk.politics.mideast']\n",
    "# Download the training subset of the 20 NG dataset, with headers, footers, quotes removed\n",
    "# Only keep docs from the 6 categories above\n",
    "ng_train = datasets.fetch_20newsgroups(subset='train', categories=categories, \n",
    "                                      remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'\\n\\n\\nI happen to be a big fan of Jayson Stark.  He is a baseball writer for the \\nPhiladelphia Inquirer.  Every tuesday he writes a \"Week in Review\" column.  \\nHe writes about unusual situations that occured during the week.  Unusual\\nstats.  He has a section called \"Kinerisms of the Week\" which are stupid\\nlines by Mets brodcaster Ralph Kiner.  Every year he has the LGTGAH contest.\\nThat stands for \"Last guy to get a hit.\"  He also writes for Baseball \\nAmerica.  That column is sort of a highlights of \"Week in Review.\"  If you \\ncan, check his column out sometime.  He might make you laugh.\\n\\nRob Koffler\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the first doc\n",
    "ng_train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Preprocessing\n",
    "We'll need to generate a term-document matrix of word (token) counts for use in LDA.\n",
    "\n",
    "We'll use `sklearn`'s `CountVectorizer` to generate our term-document matrix of counts. We'll make use of a few parameters to accomplish the following preprocessing of the text documents all within the `CountVectorizer`:\n",
    "* `analyzer=word`: Tokenize by word\n",
    "* `ngram_range=(1,2)`: Keep all 1 and 2-word grams\n",
    "* `stop_words=english`: Remove all English stop words\n",
    "* `token_pattern=\\\\b[a-z][a-z]+\\\\b`: Match all tokens with 2 or more (strictly) alphabet characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='\\\\b[a-z][a-z]+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "count_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1,2),\n",
    "                                  stop_words='english', \n",
    "                                  token_pattern='\\\\b[a-z][a-z]+\\\\b')\n",
    "count_vectorizer.fit(ng_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118117, 1661)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "ng_vecs = count_vectorizer.transform(ng_train.data).transpose()\n",
    "ng_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to gensim\n",
    "We need to convert our sparse `scipy` matrix to a `gensim`-friendly object called a Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map matrix rows to words (tokens)\n",
    "We need to save a mapping (dict) of row id to word (token) for later use by gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15077"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "At this point we can simply plow ahead in creating an LDA model.  It requires our corpus of word counts, mapping of row ids to words, and the number of topics (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create lda model (equivalent to \"fit\" in sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what happened.  Here are the 5 most important words for each of the 3 topics we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.004*jesus + 0.002*matthew + 0.002*fallacy + 0.001*conclusion + 0.001*christian'),\n",
       " (1, u'0.006*jpeg + 0.003*pub + 0.002*display + 0.002*faq + 0.002*processing'),\n",
       " (2,\n",
       "  u'0.002*morris + 0.002*alomar + 0.001*career + 0.001*double + 0.001*clemens')]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Space\n",
    "If we want to map our documents to the topic space we need to actually use the LdaModel transformer that we created above, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the documents' topic vectors in a list so we can take a peak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the document vectors in the topic space, which are measures of the component of each document along each topic.  Thus, at most a document vector can have num_topics=3 nonzero components in the topic space, and most have far fewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.14074902976825285),\n",
       "  (1, 0.84829616315399592),\n",
       "  (2, 0.010954807077751436)],\n",
       " [(0, 0.030234919130326277),\n",
       "  (1, 0.031789655630704947),\n",
       "  (2, 0.93797542523896871)],\n",
       " [(0, 0.76029455927147815),\n",
       "  (1, 0.22872091684900533),\n",
       "  (2, 0.010984523879516397)],\n",
       " [(0, 0.82219006971303565),\n",
       "  (1, 0.091629552860028857),\n",
       "  (2, 0.086180377426935559)],\n",
       " [(0, 0.017264908074076341),\n",
       "  (1, 0.017108573144132416),\n",
       "  (2, 0.96562651878179118)]]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the document vectors in the topic space for the first 5 documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\\n\\n\\nI happen to be a big fan of Jayson Stark.  He is a baseball writer for the \\nPhiladelphia Inquirer.  Every tuesday he writes a \"Week in Review\" column.  \\nHe writes about unusual situations that occured during the week.  Unusual\\nstats.  He has a section called \"Kinerisms of the Week\" which are stupid\\nlines by Mets brodcaster Ralph Kiner.  Every year he has the LGTGAH contest.\\nThat stands for \"Last guy to get a hit.\"  He also writes for Baseball \\nAmerica.  That column is sort of a highlights of \"Week in Review.\"  If you \\ncan, check his column out sometime.  He might make you laugh.\\n\\nRob Koffler\\n',\n",
       " u'\\nHere\\'s one I remember: (sort of)\\nYogi\\'s asleep in a hotel room late at night and gets a call from someone.\\nAfter he answers the phone the person at the other end asks if he woke Yogi\\nup. Yogi answered, \"No, the phone did.\"',\n",
       " u'\\n\\n\\tSorry, I was, but I somehow have misplaced my diskette from the last \\ncouple of months or so. However, thanks to the efforts of Bobby, it is being \\nreplenished rather quickly!  \\n\\n\\tHere is a recent favorite:\\n\\n\\t--\\n\\n\\n       \"Satan and the Angels do not have freewill.  \\n        They do what god tells them to do. \"\\n\\n        S.N. Mozumder (snm6394@ultb.isc.rit.edu) \\n\\n\\n--\\n\\n\\n       \"Satan and the Angels do not have freewill.  \\n        They do what god tells them to do. \"',\n",
       " u'     Since when does atheism mean trashing other religions?There must be a God\\n     of inbreeding to which you are his only son.',\n",
       " u'\\n\\n\\n\\n\\nThese people were very silly.  Any team that gets to the World Series\\ncan win the World Series, and anybody who ever expects a sweep is\\ncrazy.  If you put the best team in baseball in the Series against\\nthe worst team in baseball, the worst team would win at least a game\\nmost of the time and very well could win the Series, though the odds\\nwould certainly be against them.']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng_train.data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On your own...\n",
    "- Go get some of the NIPS papers from [here](https://archive.ics.uci.edu/ml/datasets/Bag+of+Words).  \n",
    "- Try performing LDA on this data with gensim\n",
    "- Play with some of the preprocessing options and parameters for LDA, observe what happens\n",
    "- See if you can use the resulting topic space to extract topic vectors and cluster some documents\n",
    "- How do your results look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
